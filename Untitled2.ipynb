{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWMghUloCrsx"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import soundfile as sf\n",
        "from tqdm.notebook import tqdm  # use notebook version of tqdm for Jupyter\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import argparse\n",
        "import json\n",
        "import yaml\n",
        "from pprint import pprint\n",
        "import look2hear.datas\n",
        "import look2hear.models\n",
        "import look2hear.system\n",
        "import look2hear.losses\n",
        "import look2hear.metrics\n",
        "import look2hear.utils\n",
        "from look2hear.system import make_optimizer\n",
        "from dataclasses import dataclass\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, RichProgressBar\n",
        "from pytorch_lightning.callbacks.progress.rich_progress import *\n",
        "from rich.console import Console\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.loggers.wandb import WandbLogger\n",
        "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
        "from rich import print, reconfigure\n",
        "from collections.abc import MutableMapping\n",
        "from look2hear.utils import print_only, MyRichProgressBar, RichProgressBarTheme\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "# Assuming `AudioLightningModule`, `PITLossWrapper`, `TDANet` are defined somewhere else in your project\n",
        "from my_project import AudioLightningModule, PITLossWrapper, TDANet\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from rich import print"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_one_dir(in_data_dir, out_dir, data_type, spk):\n",
        "    \"\"\"Create .json file for one condition.\"\"\"\n",
        "    file_infos = []\n",
        "    in_dir = os.path.abspath(os.path.join(in_data_dir, data_type, spk))\n",
        "    wav_list = os.listdir(in_dir)\n",
        "    wav_list.sort()\n",
        "    for wav_file in tqdm(wav_list, desc=f'Processing {data_type}/{spk}'):\n",
        "        if not wav_file.endswith(\".wav\"):\n",
        "            continue\n",
        "        wav_path = os.path.join(in_dir, wav_file)\n",
        "        samples = sf.SoundFile(wav_path)\n",
        "        file_infos.append((wav_path, len(samples)))\n",
        "    if not os.path.exists(os.path.join(out_dir, data_type)):\n",
        "        os.makedirs(os.path.join(out_dir, data_type))\n",
        "    with open(os.path.join(out_dir, data_type, spk + \".json\"), \"w\") as f:\n",
        "        json.dump(file_infos, f, indent=4)\n"
      ],
      "metadata": {
        "id": "EcXvoKcWP6ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_librimix_audio(in_dir, out_dir):\n",
        "    \"\"\"Create .json files for all conditions.\"\"\"\n",
        "    speaker_list = [\"mix_clean\", \"s1\", \"s2\"]\n",
        "    for data_type in [\"train-100\", \"dev\", \"test\"]:\n",
        "        for spk in speaker_list:\n",
        "            preprocess_one_dir(in_dir, out_dir, data_type, spk)\n"
      ],
      "metadata": {
        "id": "CWbAWTnHP9TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normally, you would use argparse for command line arguments,\n",
        "# but in Jupyter, you would manually set your arguments like this:\n",
        "\n",
        "in_dir = '/path/to/input/directory'  # replace with your actual input directory path\n",
        "out_dir = '/path/to/output/directory' # replace with your actual output directory path\n"
      ],
      "metadata": {
        "id": "sXLsbkcBP_tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CcBcTlbaRpJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from audioop import bias\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from .base_model import BaseModel\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n",
        "    if drop_prob == 0.0 or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "\n",
        "    shape = (x.shape[0],) + (1,) * (\n",
        "        x.ndim - 1\n",
        "    )  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "class _LayerNorm(nn.Module):\n",
        "    \"\"\"Layer Normalization base class.\"\"\"\n",
        "\n",
        "    def __init__(self, channel_size):\n",
        "        super(_LayerNorm, self).__init__()\n",
        "        self.channel_size = channel_size\n",
        "        self.gamma = nn.Parameter(torch.ones(channel_size), requires_grad=True)\n",
        "        self.beta = nn.Parameter(torch.zeros(channel_size), requires_grad=True)\n",
        "\n",
        "    def apply_gain_and_bias(self, normed_x):\n",
        "        \"\"\" Assumes input of size `[batch, chanel, *]`. \"\"\"\n",
        "        return (self.gamma * normed_x.transpose(1, -1) + self.beta).transpose(1, -1)\n",
        "\n",
        "\n",
        "def GlobLN(nOut):\n",
        "    return nn.GroupNorm(1, nOut, eps=1e-8)\n",
        "\n",
        "class ConvNormAct(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the convolution layer with normalization and a PReLU\n",
        "    activation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nIn, nOut, kSize, stride=1, groups=1):\n",
        "        \"\"\"\n",
        "        :param nIn: number of input channels\n",
        "        :param nOut: number of output channels\n",
        "        :param kSize: kernel size\n",
        "        :param stride: stride rate for down-sampling. Default is 1\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        padding = int((kSize - 1) / 2)\n",
        "        self.conv = nn.Conv1d(\n",
        "            nIn, nOut, kSize, stride=stride, padding=padding, bias=True, groups=groups\n",
        "        )\n",
        "        self.norm = GlobLN(nOut)\n",
        "        self.act = nn.PReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.conv(input)\n",
        "        output = self.norm(output)\n",
        "        return self.act(output)\n",
        "\n",
        "class ConvNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the convolution layer with normalization and PReLU activation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nIn, nOut, kSize, stride=1, groups=1, bias=True):\n",
        "        \"\"\"\n",
        "        :param nIn: number of input channels\n",
        "        :param nOut: number of output channels\n",
        "        :param kSize: kernel size\n",
        "        :param stride: stride rate for down-sampling. Default is 1\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        padding = int((kSize - 1) / 2)\n",
        "        self.conv = nn.Conv1d(\n",
        "            nIn, nOut, kSize, stride=stride, padding=padding, bias=bias, groups=groups\n",
        "        )\n",
        "        self.norm = GlobLN(nOut)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.conv(input)\n",
        "        return self.norm(output)\n",
        "\n",
        "class NormAct(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines a normalization and PReLU activation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nOut):\n",
        "        \"\"\"\n",
        "        :param nOut: number of output channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # self.norm = nn.GroupNorm(1, nOut, eps=1e-08)\n",
        "        self.norm = GlobLN(nOut)\n",
        "        self.act = nn.PReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.norm(input)\n",
        "        return self.act(output)\n",
        "\n",
        "class DilatedConv(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the dilated convolution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nIn, nOut, kSize, stride=1, d=1, groups=1):\n",
        "        \"\"\"\n",
        "        :param nIn: number of input channels\n",
        "        :param nOut: number of output channels\n",
        "        :param kSize: kernel size\n",
        "        :param stride: optional stride rate for down-sampling\n",
        "        :param d: optional dilation rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(\n",
        "            nIn,\n",
        "            nOut,\n",
        "            kSize,\n",
        "            stride=stride,\n",
        "            dilation=d,\n",
        "            padding=((kSize - 1) // 2) * d,\n",
        "            groups=groups,\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.conv(input)\n",
        "\n",
        "class DilatedConvNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the dilated convolution with normalized output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nIn, nOut, kSize, stride=1, d=1, groups=1):\n",
        "        \"\"\"\n",
        "        :param nIn: number of input channels\n",
        "        :param nOut: number of output channels\n",
        "        :param kSize: kernel size\n",
        "        :param stride: optional stride rate for down-sampling\n",
        "        :param d: optional dilation rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(\n",
        "            nIn,\n",
        "            nOut,\n",
        "            kSize,\n",
        "            stride=stride,\n",
        "            dilation=d,\n",
        "            padding=((kSize - 1) // 2) * d,\n",
        "            groups=groups,\n",
        "        )\n",
        "        # self.norm = nn.GroupNorm(1, nOut, eps=1e-08)\n",
        "        self.norm = GlobLN(nOut)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.conv(input)\n",
        "        return self.norm(output)\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_size, drop=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = ConvNorm(in_features, hidden_size, 1, bias=False)\n",
        "        self.dwconv = nn.Conv1d(\n",
        "            hidden_size, hidden_size, 5, 1, 2, bias=True, groups=hidden_size\n",
        "        )\n",
        "        self.act = nn.ReLU()\n",
        "        self.fc2 = ConvNorm(hidden_size, in_features, 1, bias=False)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.dwconv(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, in_channels, max_length):\n",
        "        pe = torch.zeros(max_length, in_channels)\n",
        "        position = torch.arange(0, max_length).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            (\n",
        "                torch.arange(0, in_channels, 2, dtype=torch.float)\n",
        "                * -(math.log(10000.0) / in_channels)\n",
        "            )\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)]\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, in_channels, n_head, dropout, is_casual):\n",
        "        super().__init__()\n",
        "        self.pos_enc = PositionalEncoding(in_channels, 10000)\n",
        "        self.attn_in_norm = nn.LayerNorm(in_channels)\n",
        "        self.attn = nn.MultiheadAttention(in_channels, n_head, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = nn.LayerNorm(in_channels)\n",
        "        self.is_casual = is_casual\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        attns = None\n",
        "        output = self.pos_enc(self.attn_in_norm(x))\n",
        "        output, _ = self.attn(output, output, output)\n",
        "        output = self.norm(output + self.dropout(output))\n",
        "        return output.transpose(1, 2)\n",
        "\n",
        "class GlobalAttention(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, drop_path) -> None:\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(out_chan, 8, 0.1, False)\n",
        "        self.mlp = Mlp(out_chan, out_chan * 2, drop=0.1)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(x))\n",
        "        x = x + self.drop_path(self.mlp(x))\n",
        "        return x\n",
        "\n",
        "class LA(nn.Module):\n",
        "    def __init__(self, inp: int, oup: int, kernel: int = 1) -> None:\n",
        "        super().__init__()\n",
        "        groups = 1\n",
        "        if inp == oup:\n",
        "            groups = inp\n",
        "        self.local_embedding = ConvNorm(inp, oup, kernel, groups=groups, bias=False)\n",
        "        self.global_embedding = ConvNorm(inp, oup, kernel, groups=groups, bias=False)\n",
        "        self.global_act = ConvNorm(inp, oup, kernel, groups=groups, bias=False)\n",
        "        self.act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x_l, x_g):\n",
        "        \"\"\"\n",
        "        x_g: global features\n",
        "        x_l: local features\n",
        "        \"\"\"\n",
        "        B, N, T = x_l.shape\n",
        "        local_feat = self.local_embedding(x_l)\n",
        "\n",
        "        global_act = self.global_act(x_g)\n",
        "        sig_act = F.interpolate(self.act(global_act), size=T, mode=\"nearest\")\n",
        "\n",
        "        global_feat = self.global_embedding(x_g)\n",
        "        global_feat = F.interpolate(global_feat, size=T, mode=\"nearest\")\n",
        "\n",
        "        out = local_feat * sig_act + global_feat\n",
        "        return out\n",
        "\n",
        "\n",
        "class UConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the block which performs successive downsampling and\n",
        "    upsampling in order to be able to analyze the input features in multiple\n",
        "    resolutions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, out_channels=128, in_channels=512, upsampling_depth=4):\n",
        "        super().__init__()\n",
        "        self.proj_1x1 = ConvNormAct(out_channels, in_channels, 1, stride=1, groups=1)\n",
        "        self.depth = upsampling_depth\n",
        "        self.spp_dw = nn.ModuleList()\n",
        "        self.spp_dw.append(\n",
        "            DilatedConvNorm(\n",
        "                in_channels, in_channels, kSize=5, stride=1, groups=in_channels, d=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "        for i in range(1, upsampling_depth):\n",
        "            if i == 0:\n",
        "                stride = 1\n",
        "            else:\n",
        "                stride = 2\n",
        "            self.spp_dw.append(\n",
        "                DilatedConvNorm(\n",
        "                    in_channels,\n",
        "                    in_channels,\n",
        "                    kSize=2 * stride + 1,\n",
        "                    stride=stride,\n",
        "                    groups=in_channels,\n",
        "                    d=1,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.res_conv = nn.Conv1d(in_channels, out_channels, 1)\n",
        "\n",
        "        self.globalatt = GlobalAttention(\n",
        "            in_channels * upsampling_depth, in_channels, 0.1\n",
        "        )\n",
        "        self.last_layer = nn.ModuleList([])\n",
        "        for i in range(self.depth - 1):\n",
        "            self.last_layer.append(LA(in_channels, in_channels, 5))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: input feature map\n",
        "        :return: transformed feature map\n",
        "        \"\"\"\n",
        "        residual = x.clone()\n",
        "        # Reduce --> project high-dimensional feature maps to low-dimensional space\n",
        "        output1 = self.proj_1x1(x)\n",
        "        output = [self.spp_dw[0](output1)]\n",
        "\n",
        "        # Do the downsampling process from the previous level\n",
        "        for k in range(1, self.depth):\n",
        "            out_k = self.spp_dw[k](output[-1])\n",
        "            output.append(out_k)\n",
        "\n",
        "        # global features\n",
        "        global_f = []\n",
        "        for fea in output:\n",
        "            global_f.append(F.adaptive_avg_pool1d(\n",
        "                fea, output_size=output[-1].shape[-1]\n",
        "            ))\n",
        "        global_f = self.globalatt(torch.stack(global_f, dim=1).sum(1))  # [B, N, T]\n",
        "\n",
        "        x_fused = []\n",
        "        # Gather them now in reverse order\n",
        "        for idx in range(self.depth):\n",
        "            tmp = F.interpolate(global_f, size=output[idx].shape[-1], mode=\"nearest\") + output[idx]\n",
        "            x_fused.append(tmp)\n",
        "\n",
        "        expanded = None\n",
        "        for i in range(self.depth - 2, -1, -1):\n",
        "            if i == self.depth - 2:\n",
        "                expanded = self.last_layer[i](x_fused[i], x_fused[i - 1])\n",
        "            else:\n",
        "                expanded = self.last_layer[i](x_fused[i], expanded)\n",
        "        return self.res_conv(expanded) + residual\n",
        "\n",
        "\n",
        "class Recurrent(nn.Module):\n",
        "    def __init__(self, out_channels=128, in_channels=512, upsampling_depth=4, _iter=4):\n",
        "        super().__init__()\n",
        "        self.unet = UConvBlock(out_channels, in_channels, upsampling_depth)\n",
        "        self.iter = _iter\n",
        "        # self.attention = Attention_block(out_channels)\n",
        "        self.concat_block = nn.Sequential(\n",
        "            nn.Conv1d(out_channels, out_channels, 1, 1, groups=out_channels), nn.PReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        mixture = x.clone()\n",
        "        for i in range(self.iter):\n",
        "            if i == 0:\n",
        "                x = self.unet(x)\n",
        "            else:\n",
        "                x = self.unet(self.concat_block(mixture + x))\n",
        "        return x\n",
        "\n",
        "class WaveNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size):\n",
        "        super(WaveNetBlock, self).__init__()\n",
        "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size // 2)\n",
        "        self.bn = nn.BatchNorm1d(out_channels)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return self.activation(x)\n",
        "\n",
        "class TDANet(BaseModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        out_channels=128,\n",
        "        in_channels=512,\n",
        "        num_blocks=16,\n",
        "        upsampling_depth=4,\n",
        "        enc_kernel_size=21,\n",
        "        num_sources=2,\n",
        "        sample_rate=16000,\n",
        "        # New parameters for dilated convolutions\n",
        "        dilation_rates=[1, 2, 4, 8],  # dilation rates for different layers\n",
        "    ):\n",
        "        super(TDANet, self).__init__(sample_rate=sample_rate)\n",
        "\n",
        "        # Number of sources to produce\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_blocks = num_blocks\n",
        "        self.upsampling_depth = upsampling_depth\n",
        "        self.enc_kernel_size = enc_kernel_size * sample_rate // 1000\n",
        "        self.enc_num_basis = self.enc_kernel_size // 2 + 1\n",
        "        self.num_sources = num_sources\n",
        "\n",
        "        # Appropriate padding is needed for arbitrary lengths\n",
        "        self.lcm = abs(\n",
        "            self.enc_kernel_size // 4 * 4 ** self.upsampling_depth\n",
        "        ) // math.gcd(self.enc_kernel_size // 4, 4 ** self.upsampling_depth)\n",
        "\n",
        "        # Front end\n",
        "        self.encoder = nn.ModuleList([\n",
        "            nn.Conv1d(\n",
        "                in_channels=1 if i == 0 else self.enc_num_basis,\n",
        "                out_channels=self.enc_num_basis,\n",
        "                kernel_size=self.enc_kernel_size,\n",
        "                stride=self.enc_kernel_size // 4,\n",
        "                padding=self.enc_kernel_size // 2,\n",
        "                dilation=dilation_rates[i],\n",
        "                bias=False\n",
        "            ) for i in range(len(dilation_rates))\n",
        "        ])\n",
        "        for conv in self.encoder:\n",
        "            torch.nn.init.xavier_uniform_(conv.weight)\n",
        "\n",
        "        # Norm before the rest, and apply one more dense layer\n",
        "        self.ln = GlobLN(self.enc_num_basis)\n",
        "        self.bottleneck = nn.Conv1d(\n",
        "            in_channels=self.enc_num_basis, out_channels=out_channels, kernel_size=1\n",
        "        )\n",
        "\n",
        "        # Separation module\n",
        "        self.sm = Recurrent(out_channels, in_channels, upsampling_depth, num_blocks)\n",
        "        self.sm_bn = nn.ModuleList([nn.BatchNorm1d(out_channels) for _ in range(num_blocks)])\n",
        "\n",
        "        # Back end with WaveNet-like decoder blocks (Modification 10)\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            WaveNetBlock(self.enc_num_basis * num_sources, self.enc_num_basis * num_sources, kernel_size=3)\n",
        "            for _ in range(upsampling_depth)\n",
        "        ])\n",
        "\n",
        "        self.final_decoder = nn.ConvTranspose1d(\n",
        "            in_channels=self.enc_num_basis * num_sources,\n",
        "            out_channels=num_sources,\n",
        "            kernel_size=self.enc_kernel_size,\n",
        "            stride=self.enc_kernel_size // 4,\n",
        "            padding=self.enc_kernel_size // 2,\n",
        "            groups=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        torch.nn.init.xavier_uniform_(self.final_decoder.weight)\n",
        "\n",
        "        self.mask_nl_class = nn.ReLU()\n",
        "\n",
        "    def pad_input(self, input, window, stride):\n",
        "        \"\"\"\n",
        "        Zero-padding input according to window/stride size.\n",
        "        \"\"\"\n",
        "        batch_size, nsample = input.shape\n",
        "\n",
        "        # pad the signals at the end for matching the window/stride size\n",
        "        rest = window - (stride + nsample % window) % window\n",
        "        if rest > 0:\n",
        "            pad = torch.zeros(batch_size, rest).type(input.type())\n",
        "            input = torch.cat([input, pad], 1)\n",
        "        pad_aux = torch.zeros(batch_size, window - stride).type(input.type())\n",
        "        input = torch.cat([pad_aux, input, pad_aux], 1)\n",
        "\n",
        "        return input, rest\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, input_wav):\n",
        "        # input shape: (B, T)\n",
        "        was_one_d = False\n",
        "        if input_wav.ndim == 1:\n",
        "            was_one_d = True\n",
        "            input_wav = input_wav.unsqueeze(0)\n",
        "        if input_wav.ndim == 2:\n",
        "            input_wav = input_wav\n",
        "        if input_wav.ndim == 3:\n",
        "            input_wav = input_wav.squeeze(1)\n",
        "\n",
        "        x, rest = self.pad_input(\n",
        "            input_wav, self.enc_kernel_size, self.enc_kernel_size // 4\n",
        "        )\n",
        "        # Front end\n",
        "        x = self.encoder(x.unsqueeze(1))\n",
        "\n",
        "        # Split paths\n",
        "        s = x.clone()\n",
        "        # Separation module\n",
        "        x = self.ln(x)\n",
        "        x = self.bottleneck(x)\n",
        "        x = self.sm(x)\n",
        "\n",
        "        x = self.mask_net(x)\n",
        "        x = x.view(x.shape[0], self.num_sources, self.enc_num_basis, -1)\n",
        "        x = self.mask_nl_class(x)\n",
        "        x = x * s.unsqueeze(1)\n",
        "        # Back end\n",
        "        estimated_waveforms = self.decoder(x.view(x.shape[0], -1, x.shape[-1]))\n",
        "        estimated_waveforms = estimated_waveforms[\n",
        "            :,\n",
        "            :,\n",
        "            self.enc_kernel_size\n",
        "            - self.enc_kernel_size\n",
        "            // 4 : -(rest + self.enc_kernel_size - self.enc_kernel_size // 4),\n",
        "        ].contiguous()\n",
        "        if was_one_d:\n",
        "            return estimated_waveforms.squeeze(0)\n",
        "        return estimated_waveforms\n",
        "\n",
        "    def get_model_args(self):\n",
        "        model_args = {\"n_src\": 2}\n",
        "        return model_args\n"
      ],
      "metadata": {
        "id": "f4n5O-KnQBao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the TDANet model\n",
        "model = TDANet(\n",
        "    out_channels=128,\n",
        "    in_channels=512,\n",
        "    num_blocks=16,\n",
        "    upsampling_depth=4,\n",
        "    enc_kernel_size=21,\n",
        "    num_sources=2,\n",
        "    sample_rate=16000,\n",
        ")\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 100\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (input_wav, target_wav) in enumerate(train_loader):\n",
        "        input_wav, target_wav = input_wav.to(device), target_wav.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output_wav = model(input_wav)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output_wav, target_wav)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    # Optional: Save the model checkpoint\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        torch.save(model.state_dict(), f'checkpoint_epoch_{epoch+1}.pth')"
      ],
      "metadata": {
        "id": "zcFEzOTyRFzw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}